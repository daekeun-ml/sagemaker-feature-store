{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection with Amazon SageMaker FeatureStore\n",
    "\n",
    "***본 노트북 코드는 [영문 노트북](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-featurestore/sagemaker_featurestore_fraud_detection_python_sdk.ipynb)의 번역본으로 직역이 아닌 중간중간 설명을 덧붙이고 코드를 서울(ICN) 리전에서도 수행 가능하도록 일부 수정했습니다.***\n",
    "\n",
    "이 코드는 SageMaker Studio의 `Python 3 (Data Science)` 커널과 SageMaker의 `conda_python3` 에서 정상 동작합니다.\n",
    "또한, 코드를 원활하게 실행하려면 SageMaker Feature Store에 대한 IAM 권한(`AmazonSageMakerFeatureStoreAccess`)을 추가로 부여해야 합니다.\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. [Background](#Background)\n",
    "1. [Setup SageMaker FeatureStore](#Setup-SageMaker-FeatureStore)\n",
    "1. [Inspect Dataset](#Inspect-Dataset)\n",
    "1. [Ingest Data into FeatureStore](#Ingest-Data-into-FeatureStore)\n",
    "1. [Build_Training_Dataset](#Build-Training-Dataset)\n",
    "1. [Train_and Deploy_the Model](#Train-and-Deploy-the-Model)\n",
    "1. [SageMaker FeatureStore At Inference](#SageMaker-FeatureStore-During-Inference)\n",
    "1. [Cleanup Resources](#Cleanup-Resources)\n",
    "\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker FeatureStore는 고객이 ML (머신 러닝) 개발을 위해 선별된 데이터를 쉽게 생성하고 관리할 수 있도록 해주는 신규 SageMaker 기능입니다. SageMaker FeatureStore는 높은 TPS API를 통한 데이터 수집과 온라인 및 오프라인 스토어를 통한 데이터 소비를 지원합니다.\n",
    "\n",
    "이 노트북은 사기 탐지(fraud detection) 모델 훈련 과정을 통해 SageMaker FeatureStore에서 제공하는 API에 대한 예제를 제공합니다. 노트북은 데이터 세트의 테이블을 FeatureStore로 수집하고, 훈련 데이터 세트를 생성하도록 쿼리하고, 추론 중에 빠르게 액세스하는 방법을 보여줍니다.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "**FeatureGroup**은 SageMaker FeatureStore에 저장된 모든 데이터에 대한 메타 데이터를 포함하는 기본 리소스입니다. FeatureGroup에는 FeatureDefinitions 목록이 포함됩니다. **FeatureDefinition**은 이름과 정수, 문자열 또는 십진수와 같은 데이터 유형 중 하나로 구성됩니다. FeatureGroup에는 데이터가 저장되는 위치를 제어하는 **OnlineStoreConfig** 및 **OfflineStoreConfig**도 포함됩니다. 온라인 스토어를 활성화하면 GetRecord API를 통해 레코드의 최신 값에 빠르게 액세스할 수 있습니다. 필수 구성 인 오프라인 저장소를 사용하면 S3 버켓에 과거 데이터를 저장할 수 있습니다.\n",
    "\n",
    "FeatureGroup이 생성되면 데이터를 Record로 추가할 수 있습니다. **Records**는 테이블의 행으로 생각할 수 있습니다. 각 레코드에는 FeatureGroup의 다른 모든 FeatureDefinitions에 대한 값과 함께 고유한 **RecordIdentifier**가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker FeatureStore\n",
    "\n",
    "SageMaker Python SDK 및 boto 클라이언트를 설정하여 시작하겠습니다. SageMaker Python SDK는 2020년 10월부터 V2로 업데이트되었으며, V1 대비 변경점이 많기 때문에 V1 용법에 익숙하시다면 개발자 문서를 참조하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "original_boto3_version = boto3.__version__\n",
    "%pip install 'boto3>1.17.21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "# Feature Store는 sagemaker-featurestore-runtime 서비스명으로 세션을 새로 생성합니다.\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OfflineStore에 대한 S3 버켓 설정\n",
    "\n",
    "SageMaker FeatureStore는 FeatureGroup의 오프라인 스토어 있는 데이터를 사용자 소유의 S3 버켓에 기록합니다. S3 버켓에 기록할 수 있도록 SageMaker FeatureStore에 대한 액세스 권한이 있는 IAM 역할(role)을 부여해야 합니다. FeatureGroup에서 동일한 버켓을 재사용할 수 있습니다. 버켓의 데이터는 FeatureGroup으로 분할됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the following to use a bucket of your choosing\n",
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = \"sagemaker-featurestore-demo\"\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IAM 역할을 설정합니다. 이 역할은 SageMaker FeatureStore에 S3 버켓에 대한 액세스 권한을 부여합니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> 이 예제에서는 <b>AmazonSageMakerFullAccess</b> 및 <b>AmazonSageMakerFeatureStoreAccess</b> 관리형 정책이 모두 있다고 가정하고 기본 SageMaker 역할을 사용합니다. 그렇지 않은 경우 계속하기 전에 역할에 첨부했는지 확인하십시오.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
    "role = get_execution_role()\n",
    "print (role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset \n",
    "\n",
    "제공된 데이터셋은 ID 테이블과 트랜잭션 테이블로 구성된 합성 데이터셋이며, 두 테이블을 `TransactionId` 열로 조인(join)할 수 있습니다. 트랜잭션 테이블에는 금액, 신용 카드 또는 직불 카드와 같은 특정 트랜잭션에 대한 정보가 포함되고, ID 테이블에는 장치 유형 및 브라우저와 같은 사용자에 대한 정보가 포함됩니다. 트랜잭션은 트랜잭션 테이블에 있어야 하지만, ID 테이블에서 항상 사용 가능한 것은 아닙니다.\n",
    "\n",
    "모델의 목적(objective)은 거래 기록이 주어졌을 때, 거래가 사기인지 아닌지를 예측하는 것입니다. (트랜잭션 테이블의 `isFraud` 열)\n",
    "\n",
    "아래 코드 셀에서는 `sagemaker-sample-files` 공용 S3 버켓에 저장된 데이터셋을 Pandas의 데이터프레임으로 로드 후, 아래와 같은 전처리를 수행합니다.\n",
    "- 소수점 이하 5 자리까지 반올림\n",
    "- 결측값을 0으로 보완(imputation)\n",
    "- card4(카드 발급 은행), card6(카드 종류) 열을 One-Hot Encoding으로 변환 후 결합\n",
    "- 열 이름의 공백을 '_'로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "fraud_detection_bucket_name = \"sagemaker-sample-files\"\n",
    "identity_file_key = (\n",
    "    \"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_identity.csv\"\n",
    ")\n",
    "transaction_file_key = (\n",
    "    \"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_transactions.csv\"\n",
    ")\n",
    "\n",
    "identity_data_object = s3_client.get_object(\n",
    "    Bucket=fraud_detection_bucket_name, Key=identity_file_key\n",
    ")\n",
    "transaction_data_object = s3_client.get_object(\n",
    "    Bucket=fraud_detection_bucket_name, Key=transaction_file_key\n",
    ")\n",
    "\n",
    "identity_data = pd.read_csv(io.BytesIO(identity_data_object[\"Body\"].read()))\n",
    "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object[\"Body\"].read()))\n",
    "\n",
    "identity_data = identity_data.round(5)\n",
    "transaction_data = transaction_data.round(5)\n",
    "\n",
    "identity_data = identity_data.fillna(0)\n",
    "transaction_data = transaction_data.fillna(0)\n",
    "\n",
    "# Feature transformations for this dataset are applied before ingestion into FeatureStore.\n",
    "# One hot encode card4, card6\n",
    "encoded_card_bank = pd.get_dummies(transaction_data[\"card4\"], prefix=\"card_bank\")\n",
    "encoded_card_type = pd.get_dummies(transaction_data[\"card6\"], prefix=\"card_type\")\n",
    "\n",
    "transformed_transaction_data = pd.concat(\n",
    "    [transaction_data, encoded_card_type, encoded_card_bank], axis=1\n",
    ")\n",
    "# blank space is not allowed in feature name\n",
    "transformed_transaction_data = transformed_transaction_data.rename(\n",
    "    columns={\"card_bank_american express\": \"card_bank_american_express\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_transaction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_transaction_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data into FeatureStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 단계에서는 트랜잭션 및 ID 테이블을 나타내는 FeatureGroup을 생성 후, 데이터를 삽입합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define FeatureGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "identity_feature_group_name = \"identity-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "transaction_feature_group_name = \"transaction-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureGroup을 정의합니다. 정의 후, `create()` 함수를 호출해야 FeatureGroup이 실제로 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group_name = 'identity-feature-group-27-02-47-25'\n",
    "transaction_feature_group_name = 'transaction-feature-group-27-02-47-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "identity_feature_group = FeatureGroup(\n",
    "    name=identity_feature_group_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "transaction_feature_group = FeatureGroup(\n",
    "    name=transaction_feature_group_name, sagemaker_session=feature_store_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 Feature Store에서 사용하가 위해 변환 작업을 수행합니다.\n",
    "\n",
    "`load_feature_definitions()` 함수는 피쳐 정의(feature definition)를 로드하며, 각 열의 데이터 유형을 자동으로 감지합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "\n",
    "\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "\n",
    "\n",
    "# cast object dtype to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type.\n",
    "cast_object_to_string(identity_data)\n",
    "cast_object_to_string(transformed_transaction_data)\n",
    "\n",
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"TransactionID\"\n",
    "event_time_feature_name = \"EventTime\"\n",
    "\n",
    "# append EventTime feature\n",
    "identity_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(identity_data), dtype=\"float64\"\n",
    ")\n",
    "transformed_transaction_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(transaction_data), dtype=\"float64\"\n",
    ")\n",
    "\n",
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "identity_feature_group.load_feature_definitions(data_frame=identity_data)\n",
    "# output is suppressed\n",
    "transaction_feature_group.load_feature_definitions(data_frame=transformed_transaction_data)\n",
    "# output is suppressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create FeatureGroups in SageMaker FeatureStore\n",
    "\n",
    "앞 코드 셀에서 FeatureGroup이 정의되었으므로 이제 `create()` 함수로 FeatureGroup을 생성할 수 있습니다. \n",
    "\n",
    "오프라인 스토어가 디폴트로 활성화되어 있으며, `s3_uri` 인자값에서 S3 버켓 경로를 지정하시면 됩니다.\n",
    "또한, 온라인 스토어는 디폴트로 생성되지 않으므로, 만약 온라인 스토어를 활성화하려면 `enable_online_store` 인자값을 True로 설정하셔야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "\n",
    "identity_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "transaction_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=identity_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=transaction_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DescribeFeatureGroup 및 ListFeatureGroups API를 사용하여 FeatureGroup이 생성되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작성된 Feature Store는 describe에서 확인하거나 list_feature_groups에서 목록 검색이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.list_feature_groups() # use boto client to list FeatureGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[Tip]** 여기까지 실행하셨다면, 아래 코드 셀들을 실행하기 전에 SageMaker Studio의 Feature Group을 확인해 보세요. 그리고 AWS Glue에서 Databases-Tables 내역도 같이 확인해 보세요. \n",
    "\n",
    "Amazon Feature Store는 FeatureGroup 생성 시 자동으로 AWS Glue Data Catalog를 구축합니다. 따라서 곧바로 인프라 프로비저닝 없이 Athena에서 SQL 쿼리를 자유롭게 사용할 수 있습니다. AWS Glue Data Catalog 생성 옵션은 디폴트로 활성화되어 있지만, 비활성화가 가능합니다.\n",
    "\n",
    "![feature_store](feature_store.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PutRecords into FeatureGroup\n",
    "\n",
    "FeatureGroups가 생성된 후 PutRecord API를 사용하여 FeatureGroups에 데이터를 `ingest` 함수로 넣을 수 있습니다. 이 API는 높은 TPS(초당 트랜잭션)를 처리할 수 있으며 다른 스트림에서 호출되도록 설계되었습니다. 이러한 모든 Put 요청의 데이터는 버퍼링되어 청크 단위로 S3에 기록됩니다. 수집 후 몇 분 이내에 파일이 오프라인 저장소에 기록됩니다. 이 예에서는 수집 프로세스를 가속화하기 위해 작업을 동시에 수행할 여러 작업자(worker)들을 지정합니다. 2개의 FeatureGroup에 각각 데이터를 수집하는 데 약 1분 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group.ingest(\n",
    "    data_frame=identity_data, max_workers=3, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_feature_group.ingest(\n",
    "    data_frame=transformed_transaction_data, max_workers=5, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get records from a FeatureGroup\n",
    "\n",
    "데이터가 수집되었는지 확인하기 위해 온라인 상점(online store)에서 레코드를 빠르게 검색할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = str(2990130)\n",
    "\n",
    "featurestore_runtime.get_record(\n",
    "    FeatureGroupName=transaction_feature_group_name,\n",
    "    RecordIdentifierValueAsString=record_identifier_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hive DDL\n",
    "\n",
    "SageMaker Python SDK의 FeatureStore 클래스는 Hive DDL 명령을 생성하는 기능도 제공합니다. 테이블의 스키마는 피쳐 정의를 기반으로 생성됩니다. 열은 피쳐 이름에 따라 이름이 지정되고 데이터 유형은 피쳐 유형에 따라 추론됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(identity_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transaction_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glue Data Catalog\n",
    "\n",
    "SageMaker Feature Store는 오프라인 스토어의 경우 FeatureGroup이 생성될 때, Glue 데이터 카탈로그에도 자동으로 정보가 등록됩니다. (아래 스크린샷 참조)\n",
    "\n",
    "![glue](glue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터셋 생성으로 넘어가기 전에 오프라인 스토어에 데이터가 표시될 때까지 기다리겠습니다. 약 5분 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "#print(account_id)\n",
    "\n",
    "identity_feature_group_resolved_output_s3_uri = (\n",
    "    identity_feature_group.describe()\n",
    "    .get(\"OfflineStoreConfig\")\n",
    "    .get(\"S3StorageConfig\")\n",
    "    .get(\"ResolvedOutputS3Uri\")\n",
    ")\n",
    "transaction_feature_group_resolved_output_s3_uri = (\n",
    "    transaction_feature_group.describe()\n",
    "    .get(\"OfflineStoreConfig\")\n",
    "    .get(\"S3StorageConfig\")\n",
    "    .get(\"ResolvedOutputS3Uri\")\n",
    ")\n",
    "\n",
    "identity_feature_group_s3_prefix = identity_feature_group_resolved_output_s3_uri.replace(\n",
    "    f\"s3://{default_s3_bucket_name}/\", \"\"\n",
    ")\n",
    "transaction_feature_group_s3_prefix = transaction_feature_group_resolved_output_s3_uri.replace(\n",
    "    f\"s3://{default_s3_bucket_name}/\", \"\"\n",
    ")\n",
    "\n",
    "\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:\n",
    "    objects_in_bucket = s3_client.list_objects(\n",
    "        Bucket=default_s3_bucket_name, Prefix=transaction_feature_group_s3_prefix\n",
    "    )\n",
    "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
    "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
    "    else:\n",
    "        print(\"Waiting for data in offline store...\\n\")\n",
    "        sleep(60)\n",
    "\n",
    "print(\"Data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker FeatureStore는 오프라인 스토어에 수집된 각 레코드에 대한 메타 데이터를 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore는 피쳐 그룹에 대한 Glue Data Catalog를 자동으로 빌드합니다. (피쳐 그룹을 생성하는 동안 선택적으로 켜고 끌 수 있습니다). 이 예시에서는 ID와 트랜잭션 FeatureGroups 모두의 FeatureValue를 사용하여 하나의 훈련 데이터셋을 생성합니다. 이것은 자동 구축된(auto-built) 카탈로그를 활용하여 수행됩니다. \n",
    "\n",
    "데이터셋 생성 후에는 Athena SQL 쿼리로 데이터 검색이 가능합니다. 아래 코드 셀에서 2개의 FeatureGroup에서 S3의 오프라인 스토어에 저장된 데이터를 조인하는 쿼리를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_query = identity_feature_group.athena_query()\n",
    "transaction_query = transaction_feature_group.athena_query()\n",
    "\n",
    "identity_table = identity_query.table_name\n",
    "transaction_table = transaction_query.table_name\n",
    "\n",
    "query_string = (\n",
    "    'SELECT * FROM \"'\n",
    "    + transaction_table\n",
    "    + '\" LEFT JOIN \"'\n",
    "    + identity_table\n",
    "    + '\" ON \"'\n",
    "    + transaction_table\n",
    "    + '\".transactionid = \"'\n",
    "    + identity_table\n",
    "    + '\".transactionid'\n",
    ")\n",
    "print(\"Running \" + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "# dataset = pd.DataFrame()\n",
    "identity_query.run(\n",
    "    query_string=query_string,\n",
    "    output_location=\"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/query_results/\",\n",
    ")\n",
    "identity_query.wait()\n",
    "dataset = identity_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare query results for training.\n",
    "query_execution = identity_query.get_query_execution()\n",
    "query_result = (\n",
    "    \"s3://\"\n",
    "    + default_s3_bucket_name\n",
    "    + \"/\"\n",
    "    + prefix\n",
    "    + \"/query_results/\"\n",
    "    + query_execution[\"QueryExecution\"][\"QueryExecutionId\"]\n",
    "    + \".csv\"\n",
    ")\n",
    "print(query_result)\n",
    "\n",
    "# Select useful columns for training with target column as the first.\n",
    "dataset = dataset[\n",
    "    [\n",
    "        \"isfraud\",\n",
    "        \"transactiondt\",\n",
    "        \"transactionamt\",\n",
    "        \"card1\",\n",
    "        \"card2\",\n",
    "        \"card3\",\n",
    "        \"card5\",\n",
    "        \"card_type_credit\",\n",
    "        \"card_type_debit\",\n",
    "        \"card_bank_american_express\",\n",
    "        \"card_bank_discover\",\n",
    "        \"card_bank_mastercard\",\n",
    "        \"card_bank_visa\",\n",
    "        \"id_01\",\n",
    "        \"id_02\",\n",
    "        \"id_03\",\n",
    "        \"id_04\",\n",
    "        \"id_05\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Write to csv in S3 without headers and index column.\n",
    "dataset.to_csv(\"dataset.csv\", header=False, index=False)\n",
    "s3_client.upload_file(\"dataset.csv\", default_s3_bucket_name, prefix + \"/training_input/dataset.csv\")\n",
    "dataset_uri_prefix = \"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/training_input/\"\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy the Model\n",
    "\n",
    "이제 모델에 맞게 훈련 작업을 시작할 때입니다. 먼저, SageMaker XGBoost 컨테이너를 호출하고 generic SageMaker Estimator를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.0-1\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: 이전 코드 셀이 SageMaker XGBoost 훈련 이미지를 호출하지 못하는 경우, 리전의 제한된 지원 때문일 수 있습니다. 사용 가능한 리전을 찾으려면 Amazon SageMaker 개발자 안내서의 [Docker Registry Paths for SageMaker Built-in Algorithms](https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)를 참조하십시오. 설명서에 나열되지 않은 리전에 있는 경우, 아래 코드를 실행하여 사전 빌드된 SageMaker XGBoost 기본 이미지를 수동으로 가져와 [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/)로 푸시해야 합니다. SageMaker Studio 앱은 도커 컨테이너를 기반으로 실행되고 도커를 지원하지 않기 때문에, **SageMaker 노트북 인스턴스에서 이 수동 도커 등록을 사용해야 합니다.**\n",
    "\n",
    "**Step 1** : SageMaker XGBoost 컨테이너를 ECR 계정으로 가져오고 빌드하고 푸시합니다. 다음 bash 스크립트는 `us-east-2` 리전에서 SageMaker XGBoost Docker 이미지를 가져와 ECR로 푸시합니다.\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "public_ecr=257758044811.dkr.ecr.us-east-2.amazonaws.com\n",
    "image=sagemaker-xgboost\n",
    "tag=1.0-1-cpu-py3\n",
    "\n",
    "# Add the public ECR for XGBoost image to authenticated registries\n",
    "aws ecr get-login-password --region us-east-2 | \\\n",
    "    docker login --username AWS --password-stdin $public_ecr\n",
    "\n",
    "# Pull the XGBoost image\n",
    "docker pull $public_ecr/$image:$tag\n",
    "\n",
    "# Push the image to your ECR\n",
    "my_region=$(aws configure get region)\n",
    "my_account=$(aws sts get-caller-identity --query Account | tr -d '\"')\n",
    "my_ecr=$my_account.dkr.ecr.$my_region.amazonaws.com \n",
    "\n",
    "# Authenticate your ECR\n",
    "aws ecr get-login-password --region $my_region | \\\n",
    "    docker login --username AWS --password-stdin $my_ecr\n",
    "\n",
    "# Create a repository in your ECR to host the XGBoost image\n",
    "repository_name=sagemaker-xgboost\n",
    "\n",
    "if aws ecr create-repository --repository-name $repository_name ; then\n",
    "    echo \"Repository $repository_name created!\"\n",
    "else\n",
    "    echo \"Repository $repository_name already exists!\"\n",
    "fi\n",
    "\n",
    "# Push the image to your ECR\n",
    "docker tag $public_ecr/$image:$tag $my_ecr/$image:$tag\n",
    "docker push $my_ecr/$image:$tag\n",
    "```\n",
    "\n",
    "**Step 2**: ECR 이미지 URI를 사용하려면 다음 코드를 실행하고 `training_image` string object를 설정합니다.\n",
    "```python\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "ecr = '{}.dkr.ecr.{}.amazonaws.com'.format(account_id, region)\n",
    "\n",
    "training_image=ecr + '/' + 'sagemaker-xgboost:1.0-1-cpu-py3'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Estimator` 개체를 생성합니다. 이 estimator는 훈련 작업을 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker XGBoost 컨테이너를 사용하여 SageMaker generic estimator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_path = \"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/training_output\"\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "training_model = Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    volume_size=5,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=training_output_path,\n",
    "    sagemaker_session=feature_store_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라메터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.set_hyperparameters(objective=\"binary:logistic\", num_round=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 데이터셋 지정\n",
    "\n",
    "[Build Training Dataset](#Build-Training-Dataset) 섹션에서 생성한 훈련 데이터셋을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    dataset_uri_prefix,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "training_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Hosting for the Model\n",
    "\n",
    "훈련이 완료되면 훈련된 모델을 Amazon SageMaker 실시간 호스팅 엔드포인트(real-time hosted endpoint)로 배포할 수 있습니다. 이를 통해 모델에서 예측(또는 추론)을 수행할 수 있습니다. 훈련에 사용한 것과 동일한 인스턴스(또는 인스턴스 유형)에서 호스팅할 필요가 없습니다. 아래 코드를 통해 간단하게 엔드포인트를 배포 가능하며, 약 8-10분이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = training_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker FeatureStore During Inference\n",
    "\n",
    "SageMaker FeatureStore는 low-latency GetRecord 기능을 지원하기에, 추론 요청을 위한 데이터를 보완하는 데 유용할 수 있습니다. 이 데모에서는 TransactionId가 제공되고 추론 요청을 작성하기 위해 트랜잭션 데이터에 대한 온라인 FeatureGroup을 쿼리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incoming inference request.\n",
    "transaction_id = str(3450774)\n",
    "\n",
    "# Helper to parse the feature value from the record.\n",
    "def get_feature_value(record, feature_name):\n",
    "    return str(list(filter(lambda r: r[\"FeatureName\"] == feature_name, record))[0][\"ValueAsString\"])\n",
    "\n",
    "\n",
    "transaction_response = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=transaction_feature_group_name, RecordIdentifierValueAsString=transaction_id\n",
    ")\n",
    "transaction_record = transaction_response[\"Record\"]\n",
    "\n",
    "transaction_test_data = [\n",
    "    get_feature_value(transaction_record, \"TransactionDT\"),\n",
    "    get_feature_value(transaction_record, \"TransactionAmt\"),\n",
    "    get_feature_value(transaction_record, \"card1\"),\n",
    "    get_feature_value(transaction_record, \"card2\"),\n",
    "    get_feature_value(transaction_record, \"card3\"),\n",
    "    get_feature_value(transaction_record, \"card5\"),\n",
    "    get_feature_value(transaction_record, \"card_type_credit\"),\n",
    "    get_feature_value(transaction_record, \"card_type_debit\"),\n",
    "    get_feature_value(transaction_record, \"card_bank_american_express\"),\n",
    "    get_feature_value(transaction_record, \"card_bank_discover\"),\n",
    "    get_feature_value(transaction_record, \"card_bank_mastercard\"),\n",
    "    get_feature_value(transaction_record, \"card_bank_visa\"),\n",
    "]\n",
    "\n",
    "identity_response = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=identity_feature_group_name, RecordIdentifierValueAsString=transaction_id\n",
    ")\n",
    "identity_record = identity_response[\"Record\"]\n",
    "id_test_data = [\n",
    "    get_feature_value(identity_record, \"id_01\"),\n",
    "    get_feature_value(identity_record, \"id_02\"),\n",
    "    get_feature_value(identity_record, \"id_03\"),\n",
    "    get_feature_value(identity_record, \"id_04\"),\n",
    "    get_feature_value(identity_record, \"id_05\"),\n",
    "]\n",
    "\n",
    "# Join all pieces for inference request.\n",
    "inference_request = []\n",
    "inference_request.extend(transaction_test_data[:])\n",
    "inference_request.extend(id_test_data[:])\n",
    "\n",
    "inference_request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = predictor.predict(\",\".join(inference_request), initial_args={\"ContentType\": \"text/csv\"})\n",
    "prediction = json.loads(results)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group.delete()\n",
    "transaction_feature_group.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # restore original boto3 version\n",
    "# %pip install 'boto3=={}'.format(original_boto3_version)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
